\documentclass{article}
\author{Aaron Gorenstein}
\title{Bloom Filter Implementation}
\date{\today}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{noweb}
\usepackage{cleveref}
\usepackage{listing}
\usepackage{minted}
\usepackage{tikz}
% thanks: http://gnuplot.info/docs/latex_demo.pdf
\usepackage{gnuplot-lua-tikz}
\usepackage[section]{placeins}

% Thanks https://tex.stackexchange.com/questions/99809/box-or-sidebar-for-additional-text
\usepackage{wrapfig}
\usepackage{tcolorbox}

\usepackage{biblatex}
\addbibresource{library.bib}

\newenvironment{WrapText}[1][r]
  {\wrapfigure{#1}{0.5\textwidth}\tcolorbox}
  {\endtcolorbox\endwrapfigure}

\newmintedfile[completefile]{c++}{fontsize=\small,linenos=true}

\begin{document}
\maketitle

\tableofcontents

\section{Introduction}
Formal computer science---including things like algorithms and data structures, computational complexity theory,
as well as the more ``formal'' side of practical things like compilers, databases, and operating systems---is all just so fascinating.
In an academic settings, i.e., textbooks and college courses, treatments of these topics typically focus on the underlying ideas to the exclusion of practical implementations.
This is understandable: the insights of computer science are independent of the programming language, operating system, or hardware (even as some of those insights greatly influence these things!),
and there are only so many pages in a textbook, or lectures in a course.

I think, however, such treatments would benefit from practical implementations of their ideas.
This paper is the first in a hoped-for series that provides a stand-alone\footnote{In the sense that only a standard compiler and \emph{minimal} external dependencies are needed.},
practical\footnote{In the sense that the code runs and, at least sometimes, exhibits its purported behavior.}, and approachable\footnote{In the sense that, with the accompanying text, readers will find the code completely comprehensible.}
implementation of a ``textbook'' algorithm.

We will discuss Bloom filters, introduced\cite{bloom1970} over 50 years ago and still going strong.
From the perspective that this document is to supplement existing textbook discussions (usually in specialized algorithms textbooks\cite{mitzenmacher2005probability}),
we will jump as quickly as possible into a concrete implementation.
The rest of the document is as follows:
\begin{itemize}
\item In \cref{sec:theorytocode} we will provide a complete implementation of a Bloom filter in standard C++17.
\item In \cref{sec:harnesscode} we will provide a stand-alone ``driver'' program that can take command-line parameters and actually execute code using the Bloom filter, so that we might
examine the behavior of our implementation in a real environment.
\item Finally, in \cref{sec:experiments}, we will present some findings and thoughts based on using that driver.
\end{itemize}
Along the way we will have brief interludes discussing how our practical work and findings complement academic treatments, for the learner.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bloom Filter: Theory to Code}\label{sec:theorytocode}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This section ``reads along'' with a textbook description of Bloom filters\cite{mitzenmacher2005probability},
and derives code from the prose.

\subsection{Bloom Filter Implementation}\label{sec:impl}
The Bloom filter is a persistent object that summarizes a collection of data, so we expect it to have some state.
In particular, we have a sequence $A$ of $n$ bits, and a collection of $k$ different hash functions.
This can be realized with the following code:
<<Bloom Filter Implementation>>=
template <class V, size_t (*h)(uint32_t i, const V &)>
class bloom_filter {
private:
  simple_bit_set A;

public:
  size_t n = 0;
  size_t k = 0;
  bloom_filter() {}
  bloom_filter(const size_t n, const size_t k) : A(n), n(n), k(k) {}
  int count() const { return A.count(); }
  size_t size() const { return A.size(); }
  
  <<Bloom filter set>>
  <<Bloom filter test>>
};
@
To restate:
The field [[A]] is the core sequence of bits.
It's type, [[simple_bit_set]], is specified later in \cref{sec:arrayofbits}.
When we talk about ``setting'' or ``checking'' bits, we are referencing to operations on [[A]].
The parameter [[n]] is the number of bits we'll be using in our sequence.

The hash function [[h]] is provided at compile-time as a template-parameter.
Note that it doesn't just take a [[V]] (by reference), but also an index [[i]].
This [[i]] exactly corresponds to the index machinery our textbook relies on.
The idea is that [[h(0, v)]] will give a \emph{completely} different value than [[h(1, v)]],
even though [[v]] is the same between the two.
While mechanically it is a single function, we can pretend it's an arbitrary-length array of hash functions, the first parameter being the ``index'' into that array.

\paragraph{Aside}
We made some design decisions for our Bloom filter: the size and density ([[n]] and [[k]])
are runtime parameters, but the type [[V]] and hash function [[h]] are compile-time.
These considerations are (understandably) not discussed in theoretical treatments, but matter
for practical implementations.

Part of these expositions is to consider \emph{where} we might consider these questions.
A Bloom filter, and other algorithms, have \emph{theoretical} considerations (like
the expected $\epsilon$) as also practical considerations.
These include design questions, like:
What's the type-system interface for a Bloom filter?
How much is the design constrained by the particular language features?
These extend into practical considerations: For instance, alternative implementations could
be more flexible with the type-system, but at the cost of an indirect call for each hash call.

In any case, with our parameters defined, we can turn to the key functionality:
How do we add elements to our filter, and how do we test if an element was added to our filter?
For both operations we will consider a quote from our reference, and then map that to code.
Happily, at this point we will find the mapping very straightforward.

\paragraph{Adding Elements}
\begin{quote}
For each element $s\in S$, the bits $A[h_i(s)]$ are set to 1 for $1\leq i \leq k$.
A bit location can be set to 1 multiple times, but only the first change has an effect.\cite[109]{mitzenmacher2005probability}.
\end{quote}
<<Bloom filter set>>=
void set(const V &s) {
  for (size_t i = 0; i < k; i++) {
    A.set(h(i, s) % n);
  }
}
@

\paragraph{Testing an Element}
\begin{quote}
To check if an element $x$ is in $S$, we check whether all array locations $A[h_i(x)]$
for $1 \leq i \leq k$ are set to 1. If not, then clearly $x$ is not a member of $S$,\ldots.
If all $A[h_i(x)]$ are set to 1, we assume $x$ is in $S$, although we could be wrong.\cite[109]{mitzenmacher2005probability}.
\end{quote}
<<Bloom filter test>>=
bool test(const V &s) const {
  for (size_t i = 0; i < k; i++) {
    if (!A.test(h(i, s) % n)) {
      return false;
    }
  }
  return true;
}
@

For our demonstrative implementation, that is all we need! This completes the core of our implementation,
and we can wrap it in a header file.
<<bloomfilter.h>>=
% HIDDEN
#pragma once
#include "simplebitset.h"
#include <cstddef>
#include <cstdint>

<<Bloom Filter Implementation>>
@

What remains unimplemented as is the [[simple_bit_set]], as well as our definitions of [[h]] and [[V]].

\subsection{Array of Bits}\label{sec:arrayofbits}
Some languages have this built in already.
The standard C++ libraries provide a \emph{fixed-length} (at compile time) bit-array, but combining it with the dynamically-resizable [[std::vector]] container leads to a natural implementation:
<<simplebitset.h>>=
#pragma once
#include <bitset>
#include <numeric>
#include <vector>
class simple_bit_set {
private:
  static const int page_size = 64;
  typedef std::bitset<page_size> page;
  std::vector<page> data;

public:
  simple_bit_set() {}
  simple_bit_set(const int size) : data((size / page_size) + 1, 0) {}
  bool test(unsigned int i) const {
    return data[(i / page_size)].test(i % page_size);
  }
  void set(unsigned int i) {
    data[(i / page_size)].set(i % page_size);
  }
  int size() const { return data.size() * page_size; }
  int count() const {
    int r = 0;
    for (const auto& p : data) { r += p.count(); }
    return r;
  }
};
@
The one ``trick'' that may be new to implementors is how we ``translate'' [[i]] into which 64-bit [[page]] to look up, and then look up the right bit within that page.
We can also drill into another detail: the number of bits we set will always be some multiple of [[page_size]].
Consequently, we may sometimes have slightly more bits than our Bloom filter expects.
This is fine, they're simply never referenced.
This also isn't anticipated to appreciably change the performance or even the analysis, but another example of a ``rough edge'' between theory and practice.

\subsection{Hashing Scheme and Values to Hash}
We need a so-called ``good'' hash for our Bloom filter.
Hashing algorithms are a whole field into themselves, and one I hope to explore later.
For now, we will defer to what I hear is established as ``good practice'' and use the public-domain MurmurHash3 algorithm\cite{murmur3}.
This is understood to be an extremely fast and good hash, without the overhead of cryptographic guarantees.
It provides a simple C-header interface, and most conveniently provides a ``seed'' parameter that exactly matches the use of $i$ in $h_i$.

Here is our explicit hashing function:
<<Hashing Function>>=
size_t string_hash(uint32_t i, const std::string &v) {
  uint32_t hash;
  MurmurHash3_x86_32(v.c_str(), v.size(), i, &hash);
  return static_cast<size_t>(hash);
}
@
Note that implicit in our implementation is that we've also picked our type [[V]]: we'll be hashing a collection of [[std::string]]s.
For our type system, generally, we'd want to have a hash function implemented ``in tandem'' with our value-type.
This is so that it knows how to look at the whole value.
In this case, a [[std::string]] value is actually a small object holding data like the length, and a \emph{pointer} to the ``real'' content of the string.\footnote{
  In practice things are actually even a bit more complicated for [[std::string]], where sometimes due to the so-called ``short-string optimization'' there isn't a pointer in use!
}
We should hash the \emph{contents} of the pointer rather than the \emph{value} of the pointer.
To hash the values would be a bug:
In the case where we have two copies of the same string, their underlying bytes may exist in different memory locations so a naive hash would say they're \emph{different}, but we need them to be reported the same!
So our [[string_hash]] implementation takes care of telling our underlying hash algorithm which data to look at, and how, for hashing.

\paragraph{Personal Interlude} More than once in my career a colleague has said ``we need a hashing algorithm that provides good guarantees. Let's use SHA!''
In a sense, I agree: the properties promised by \emph{cryptographic} hashing functions are very nice.
My background in computational complexity reminds me always that features (usually) have a price---in this case, CPU time.
When we just want to get a nice hashing algorithm for some in-memory object, we typically don't need to worry about hackers trying to, given the hash result, figure out the actual value!

This completes all the components of our implementation.
We have a Bloom filter library!
We could say we're done here: ``in principle'' someone can go use this code and do Bloom-filter-y things with it.
We are going to do just that: harness the code into a small command-line program that is purpose-fit to run experiments.

\section{Harnessing the Code}\label{sec:harnesscode}
We want to write code to run experiments on our Bloom filter.
We'll go through the various components we need, chunk-by-chunk, and then at the end tie it all together into a single driver program.

\subsection{Generating Random Strings}
Bloom filters summarize sets of strings: so let's make lots of unique strings.
We're not going to worry about efficiency at all, but just try to expediently generate guaranteed-unique strings.
Our implementation follows a fairly standard pattern: generate stuff and deduplicate things via a set until we get to a big-enough set.
<<Create Strings>>=
const static std::string letter_set =
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";

std::vector<std::string> create_strings(size_t count, size_t len) {
  std::random_device rd;
  std::default_random_engine e1(rd());
  std::uniform_int_distribution<int> uniform_dist(0, letter_set.length()-1);

  std::unordered_set<std::string> unique_strings;
  std::string output;
  output.resize(length_of_strings);
  while (unique_strings.size() < count) {
    for (int j = 0; j < length_of_strings; j++) {
      auto k = uniform_dist(e1);
      output[j] = letter_set[k];
    }
    unique_strings.insert(output);
  }
  std::vector<std::string> data(std::begin(unique_strings),
                                std::end(unique_strings));
  return data;
}
@
This is a pretty limited generator:
There's lots of API limitations: we are fixed to create [[count]] strings all of the same length.
There is no way to specify the ``random seed'' to enable repeatability.
There are also probably lots of C++-ism improvements, like move-semantic stuff (including emplacing), as well as API changes like taking an output interator.
Note also that a small-enough length, with a big-enough count request, means we'd never terminate (the set would simply never get big enough).
But! For our purposes, despite all these limitations, it gets us enough to run our experiments.

\subsection{Measuring Time}
Bloom filters speed things up in theory.
Some of our experiments can be to see if that happens in practice.
There are myriad ways to measure time in C++, here we can have a timer-object that uses constructor and destructor semantics to time how long it takes for the computer to execute a certain scope-of-code.
<<Timer Object>>=
class timer_object {
  std::chrono::time_point<std::chrono::steady_clock> start;
  double* elapsed_seconds;
public:
  timer_object(double* elapsed_seconds) {
    this->elapsed_seconds = elapsed_seconds;
    start = std::chrono::steady_clock::now();
  }
  ~timer_object() {
    auto end = std::chrono::steady_clock::now();
    std::chrono::duration<double> elapsed = end - start;
    *elapsed_seconds = elapsed.count();
  }
};
@
For those unfamiliar with C++, the idea is that we construct the timer object (via [[timer_object(double*...)]]),
at which point it saves the the [[start]] time.
At the end of the scope, the object is automatically destroyed, and that invokes the destructor, [[~timer_object()]],
which compares the current time with the saved time and writes the difference in the externally-provided [[*elapsed_seconds]].

\subsection{Receive Input Parameters}
Our program is ultimately going to be called from the command line.
We implement code to parse a (large!) number of flags that will enable us to run our program with various options.
We can detail them here:
<<Read Command-Line Parameters>>=
int c;
while ((c = getopt(argc, argv, "dfae:n:x:l:k:b:m:rhv:")) != -1) {
  switch (c) {
    // Which experiments to run:
    case 'd': run_density_experiment = true; break;
    case 'f': run_false_positive_experiment = true; break;
    case 'a': run_application_experiment = true; break;
    
    // Parameters to the experiment
    case 'e': desired_epsilon = std::strtod(optarg, nullptr); break;
    case 'n': num_strings_in_set = std::strtol(optarg, nullptr, 0); break;
    case 'x': num_strings_not_in_set = std::strtol(optarg, nullptr, 0); break;
    case 'l': length_of_strings = std::strtol(optarg, nullptr, 0); break;
    case 'k': num_hash_functions = std::strtol(optarg, nullptr, 0); break; 
    case 'b': num_bits_per_value = std::strtod(optarg, nullptr); break; 
    case 'm': num_bits_in_filter = std::strtol(optarg, nullptr, 0); break; 
    case 'r': rounding_constant = 1; break;

    // Meta-parameters
    case 'h': print_help = true; break;
    case 'v': verbosity_level = std::strtol(optarg, nullptr, 0); break;
  }
}
@
The loop is a standard [[getopt]] interface.
Some flags are just binary: the first three indicate which experiment(s) the user is requesting to run.
The largest chunk of flags is how the user specifies, e.g., the number of strings to insert into the Bloom filter ([[num_strings_in_set]]), the number of strings to create guaranteed \emph{not} to be in the Bloom filter ([[num_strings_not_in_set]]) and things like the desired $\epsilon$ or the raw size of the Bloom filter.
Some of these flags are mutually exclusive, and most are ultimately used to construct the Bloom filter.

\subsection{Constructing the Bloom Filter}
Some of the Bloom filter parameters can be specified from the previous options, and others can be computed from them.
Here is that logic, intertwined with some more user-friendly options (like debug statements).
<<Prepare Experiments>>=
if (verbosity_level != 0) {
  print_driver_state();
}
if (num_bits_per_value == 0.0) {
  // Specify default to be "optimal".
  num_bits_per_value = -1.44 * std::log2(desired_epsilon);
}
if (num_bits_in_filter == 0) {
  num_bits_in_filter = num_strings_in_set * num_bits_per_value;
}
if (num_hash_functions == 0) {
  if (verbosity_level >= 1) {
    printf("Ideal number of hash-functions pre-rounding: %f\n", -std::log2(desired_epsilon));
  }
  num_hash_functions = size_t(-std::log2(desired_epsilon)) + rounding_constant;
}
if (verbosity_level >= 1) {
  printf("Calculated best bits per value: %f, total number of bits: %ld, number of hash functions: %ld\n",
           num_bits_per_value, num_bits_in_filter, num_hash_functions);
}
@
See how things like [[num_hash_functions]], if left uninitialized by the user, are automatically inferred to be the ``optimal'' settings given some other parameters.
The deriviation of how things are optimal is the subject of the literature that this write-up is complementing.
(That is to say, I'm not going to write how we determined the formulas for the optimal values.)

With these parameters prepared, we can finally compute our test-string-sets and Bloom filter:
<<Initialize Driver State>>=
auto total_strings = num_strings_in_set + num_strings_not_in_set;
strings = create_strings(total_strings, length_of_strings);
included_string_begin = std::begin(strings);
included_string_end = included_string_begin+num_strings_in_set;
excluded_string_begin = included_string_end;
excluded_string_end = excluded_string_begin+num_strings_not_in_set;
assert(excluded_string_end == std::end(strings));

if (verbosity_level > 1) {
  printf("will add %zu strings, test %zu strings",
    std::distance(included_string_begin, included_string_end),
    std::distance(excluded_string_begin, excluded_string_end));
}

bf = bloom_filter<std::string, string_hash>(num_bits_in_filter, num_hash_functions);
// Populate our Bloom filter set and a baseline std::set.
std::for_each(included_string_begin, included_string_end,
              [](auto &s) {
                bf.set(s);
                base_set.insert(s);
              });
@
These sets are going to be shared between whatever experiments we run.
So we can isolate out the core \emph{state} of our program: we can move past our command-line parameters and think solely about the objects we'll use for our experiments.

That state is expressed here:
<<Driver State>>=
bloom_filter<std::string, string_hash> bf;
std::set<std::string> base_set;
std::vector<std::string> strings;
std::vector<std::string>::const_iterator
  included_string_begin,
  included_string_end,
  excluded_string_begin,
  excluded_string_end;
@
Note that the four [[const_iterator]]s combine to define two distinct sequences: the first sequence being the strings \emph{in} the Bloom filter, the second sequence being those strings \emph{not} in the Bloom filter.
These are both subsequences of the vector [[strings]].

\subsection{Defining our Experiments}
As hinted in \cref{Read Command-Line Parameters} we are going to implement 3 experiments to run with our Bloom filters and sets.
Their implementations are straightforward; only the application experiment needs helper functions.
<<Run Selected Experiments>>=
if (run_density_experiment) {
  const double density =
    static_cast<double>(bf.count()) / static_cast<double>(num_bits_in_filter);
  printf("%f %zu\n", density, bf.size());
}
if (run_false_positive_experiment) {
  auto false_positives = std::count_if(excluded_string_begin, excluded_string_end,
                                       [](auto &v) { return bf.test(v); });
  double fp_rate = double(false_positives) / double(num_strings_not_in_set);
  printf("%f\n", fp_rate);
}
if (run_application_experiment) {
  double base_seconds = time_baseline();
  double bloom_seconds = time_bloom_filter();
  printf("%f %f\n", base_seconds, bloom_seconds);
}
@
Our first experiment simply reports some static properties of the already-initialized Bloom filter.
The second experiment ``just'' validates the error rate (the $\epsilon$, the false-positive rate) by seeing how many of the guaranteed-not-to-have-been-inserted-in-the-Bloom-filter strings nonetheless are reported as included.
The third is the most involved: we use that same sequence of guaranteed-not-included strings, and see how much putting a Bloom filter ``in front'' of a normal set speeds things up.
Those details follow:
<<Time Baseline>>=
double time_baseline() {
  double seconds = 0;
  size_t count = 0;
  {
    timer_object t(&seconds);
    for (auto it = excluded_string_begin; it != excluded_string_end; it++) {
      if (base_set.find(*it) != base_set.end()) {
        count++;
      }
    }
  }
  assert(!count);
  return seconds;
}
@

<<Time Bloom Filter>>=
double time_bloom_filter() {
  double seconds = 0;
  size_t count = 0;
  {
    timer_object t(&seconds);
    for (auto it = excluded_string_begin; it != excluded_string_end; it++) {
      if (bf.test(*it)) {
        if (base_set.find(*it) != base_set.end()) {
          count++;
        }
      }
    }
  }
  assert(!count);
  return seconds;
}
@
I call this ``application'' instead of, say, ``benchmarking'' because (even though of course it really is just a contrived benchmark) the intent is to express \emph{how} Bloom filters are actually used.
We might consider them alone as an object of study, but the intent is to save time by (sometimes) avoiding an expensive lookup.

\subsection{Tying it all together}
We have all of our utilities and interfaces.
Now we just need to put it all in an executable program.
At a high-level, we describe our driver like so:
<<driver.cpp>>=
<<Driver Headers>>
<<Bloom Filter Settings>>
<<Driver Utility Functions>>
<<Driver State>>
<<Time Baseline>>
<<Time Bloom Filter>>
int main(int argc, char* argv[]) {
  <<Read Command-Line Parameters>>
  <<Error-Check Parameters>>
  <<Prepare Experiments>>
  <<Initialize Driver State>>
  <<Run Selected Experiments>>
}
@
The complete listing can be found in \cref{sec:driver-complete}.
Some of the chunks (like the headers, error-checking code, printing out the ``help'' instructions, and so on) are not included in the text
and just appear in the listing.
The exposition here is hopefully enough to describe how this driver works at a high level, and the complete code is included to keep things unambiguous.

Now let's write scripts to exercise our CLI program in various ways to finally learn more about Bloom filters in practice.


<<Bloom Filter Settings>>=
% HIDDEN
double desired_epsilon = 0.0;
int num_strings_in_set = 0;
int num_strings_not_in_set = 0;
int length_of_strings = 0;
int rounding_constant = 0;
  
bool print_help = false;
int verbosity_level = 0;

size_t num_hash_functions = 0;
double num_bits_per_value = 0.0;
size_t num_bits_in_filter = 0;

bool run_density_experiment = false;
bool run_false_positive_experiment = false;
bool run_application_experiment = false;
@

<<Error-Check Parameters>>=
% HIDDEN
if (!run_density_experiment && !run_false_positive_experiment && !run_application_experiment) {
  printf("Error: no requested experiment!\n");
  return 1;
}
if (desired_epsilon < 0 || desired_epsilon >= 1) {
  printf("Error: desired_epsilon must be between 0 and 1, got %f\n", desired_epsilon);
  return 1;
}
if (num_strings_in_set < 0) {
  printf("Error: num_strings_in_set must be non-negative, got %d\n", num_strings_in_set);
  return 1;
}
if (length_of_strings <= 0) {
  printf("Error: length_of_strings must be positive, got %d\n", length_of_strings);
  return 1;
}
if (num_bits_per_value < 0) {
  printf("Error: num_bits_per_value must be positive, got %f\n", num_bits_per_value);
  return 1;
}
if (run_false_positive_experiment || run_application_experiment) {
  if (num_strings_not_in_set <= 0) {
    printf("Error, for these experiments num_strings_not_in_set must be positive, got %d\n",
           num_strings_not_in_set);
    return 1;
  }
}
@

<<Driver Utility Functions>>=
% HIDDEN
<<Create Strings>>
<<Hashing Function>>
<<Print Driver Parameters>>
<<Timer Object>>
@

<<Driver Headers>>=
% HIDDEN
#include <algorithm>
#include <cassert>
#include <chrono>
#include <cmath>
#include <cstdio>
#include <cstdlib>
#include <unordered_set>
#include <set>
#include <string>
#include <unistd.h>

#include "bloomfilter.h"
#include "datagen.h"
#include "murmur/MurmurHash3.h"
@

<<Print Driver Parameters>>=
% HIDDEN
void print_driver_state() {
  printf("parameters:\n"
    "\tprint_help=%s\n"
    "\tverbosity_level=%d\n"
    "\trun_density_experiment=%s\n"
    "\trun_false_positive_experiment=%s\n"
    "\trun_application_experiment=%s\n"
    "\tdesired_epsilon=%f\n"
    "\tnum_strings_in_set=%d\n"
    "\tnum_strings_not_in_set=%d\n"
    "\tlength_of_strings=%d\n",
    print_help ? "true" : "false", verbosity_level,
    run_density_experiment ? "true" : "false",
    run_false_positive_experiment ? "true" : "false",
    run_application_experiment ? "true" : "false",
    desired_epsilon, num_strings_in_set, num_strings_not_in_set,
    length_of_strings);
}
@

We have our complete program: we're now ready to get some empirical results!

\section{Experiments}\label{sec:experiments}
We now explore whether our driver successfully confirms theoretical results.

\subsection{Optimal Bit-Density}
As mentioned in our motivating textbook\cite[111]{mitzenmacher2005probability}, an interesting property of an ``optimal'' Bloom filter is that its density (the proportion of bits set to $1$) should approach $0.5$.
This touches on the complicated question of what is ``optimal'', but for our purposes:
Given a size $N$ collection of strings to hash and desired false-positive rate $\epsilon$, we should be able to determine a ``best'' number-of-bits $m$ and number-of-hashes $k$.

Can we validate this manifesting in our code, empirically? Yes.
We might want a chart to show that, no matter the specified $\epsilon$, number of input strings, or length of strings,
our default hash-function-count and Bloom-filter-size will yield the desired $\epsilon$ and have density very close to $0.5$.
(We're not going to be exhaustive here: this document is to illustrate that theoretical properties can be
empirically observed, not a full validation of our Bloom filter implementation.)

So let's make an interesting graph: let's vary $\epsilon$, and chart how density varies.
Here we have a terse script that runs this experiments and plots the result:
<<plot-density.sh>>=
#!/bin/bash
e=0.8
output="$(basename "$0" .sh).fig.tex"
for i in {1..10}; do
  measured=$(./driver -d -e "$e" -n 100000 -l 24 -r)
  echo "$e $measured"
  e=$(echo "$e / 2" | bc -l)
done | gnuplot -e "
  set terminal tikz size 3.5in,2.4in;
  set xlabel 'Specified $\epsilon$';
  set xrange [*:1];
  set logscale x;
  set ylabel 'Bit Density';
  set output '$output';
  plot '-' using 1:2 with linespoints notitle"
@
The desired epsilon ([[e]]) starts at $0.8$, and on each iteration ([[e=$(echo "$e / 2" | bc -l)]]) halves it: so we're running our experiment with $\epsilon\in\{0.8,0.4,0.2,0.1,0.05\ldots\}$.
(Note that the X-axis may seem sort of ``backwards'': the bottom-left is showing that with very-low $\epsilon$, i.e., a ``better'' Bloom filter, whereas ones intuition may suggest our chart starts with a ``worse'' Bloom filter on the left.)
\begin{figure}[ht]
    \centering
    \input{plot-density.fig.tex}
    \caption{Density by Desired Epsilon}
    \label{fig:plot_density}
\end{figure}
The result in \cref{fig:plot_density} confirms that, as we request smaller and smaller $\epsilon$, the resulting bit-density approaches $0.5$.
It doesn't quite approach $0.5$ due to rounding.

Intuition---and our code in \cref{Prepare Experiments}---suggests that closer our density gets to $0.5$, the more bits we need to dedicate to each value in the Bloom filter.
(Note that, however, the number of bits dedicated to each value is wholly a function of $\epsilon$ and the total number of strings---\emph{not} the size of the strings. That's neat!)
We can confirm that with a very similar experiment: we have the exact same script as before, but instead read the next column in the output (that says the size of the Bloom filter).
<<plot-size.sh>>=
#!/bin/bash
e=0.8
output="$(basename "$0" .sh).fig.tex"
for i in {1..10}; do
  measured=$(./driver -d -e "$e" -n 100000 -l 24 -r)
  echo "$e $measured"
  e=$(echo "$e / 2" | bc -l)
done | gnuplot -e "
  set terminal tikz size 3.5in,2.4in;
  set xlabel 'Specified $\epsilon$';
  set xrange [*:1];
  set logscale x;
  set ylabel 'Filter Size';
  set output '$output';
  plot '-' using 1:3 with linespoints notitle"
@
The (log-scale, again) result in \cref{fig:plot_size} clearly confirms that our Bloom filter gets bigger the closer $\epsilon$ gets to $0$.
\begin{figure}[ht]
    \centering
    \input{plot-size.fig.tex}
    \caption{Bloom Filter Size by Desired Epsilon}
    \label{fig:plot_size}
\end{figure}

In our prior experiments we've let the driver compute, given the $\epsilon$, the ``optimal'' number of hash functions.
This invites an empirical question: how bad is it to get that wrong?
In other words, how far off does our density vary from $0.5$ if we have a typo somewhere leading to the wrong hash-function count?
We compute this as follows:
<<plot-density-per-hash.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
for hash_count in {1..40}; do
  result=$(./driver -d -e 0.0009 -n 100000 -l 24 -k "$hash_count")
  echo "$hash_count $result"
done | gnuplot -e "
  set terminal tikz size 3.5in,2.4in;
  set yrange [0:1];
  set xlabel 'Number of Hash Functions';
  set ylabel 'Bit Density';
  set output '$output';
  plot '-' using 1:2 with linespoints notitle"
@
The resulting plot shows that you can be slightly off with $k$ before it's ``obviously'' wrong.
That's interesting, and an empirical result that seems hard (to me, at least) to derive analytically.
\begin{figure}[ht]
    \centering
    \input{plot-density-per-hash.fig.tex}
    \caption{Density by Hash Function Count}
    \label{fig:density_by_hash_function_count}
\end{figure}

\subsection{False Positive Rate}
We've confirmed density.
Now, quickly: to what degree does our code empirically match the expected false-positive rate?
Basically, when we specify an epsilon, do we get that epsilon?
We can validate this with our other experiment run in our driver:
<<validate-epsilon-empirically.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
tmpfile=$(mktemp)
e=0.8
for i in {1..10}; do
  measured=$(./driver -f -e "$e" -n 100000 -x 100000 -l 24 -r)
  echo "$e $e $measured"
  e=$(echo "$e / 2" | bc -l)
done > $tmpfile

gnuplot -e "
  set terminal tikz size 3.5in,2.4in;
  set xlabel 'Specified Epsilon';
  set ylabel 'Actual Epsilon';
  set xrange [*:1];
  set logscale x;
  set yrange [*:1];
  set logscale y;
  set key at graph 0.60,0.95;
  set output '$output';
  plot '$tmpfile' using 1:2 with linespoints title 'Expected eps',
       '$tmpfile' using 1:3 with linespoints title 'Measured eps'
  "

rm $tmpfile
@

We can see that it seems to track very precisely in \cref{fig:measure_epsilon}, so that's reassuring.
\begin{figure}[ht]
    \centering
    \input{validate-epsilon-empirically.fig.tex}
    \caption{Comparing Empirical to Theoretical $\epsilon$}
    \label{fig:measure_epsilon}
\end{figure}

Interlude: one thing I realized is that, curiously, with $\epsilon > 0.5$,
the false positive rate was always (at first) exactly $1.0$.
Even for $\epsilon=0.51$ or similar.
In debugging I found that this is because we'd always round down.

The static properties of our Bloom filter seem to match our theoretical expectations.
Now: do we get the runtime improvements we expect?

\subsection{Measuring Set Speedup}
We've explored and confirmed some of the static properties of our Bloom filter.
Now let's explore the whole motivation: how much does putting a Bloom filter in front of a (slow) [[std::set]] help?
In the following experiment we fix our Bloom filter to have an $\epsilon=0.01$, request one million strings be in the set, and vary the excluded strings from $100,000$ to one million.
We run our timing (``application'') experiment, see \cref{Run Selected Experiments} on these parameters and plot how the speedup scales.
<<acceleration-by-count.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
tmpfile=$(mktemp)
for miss_count in {1..10}; do
  count="$miss_count"00000 
  result=$(./driver -a -e 0.01 -n 1000000 -x $count -l 24)
  echo "$count $result"
done > $tmpfile

gnuplot -e "
  set terminal tikz size 5in,2.4in;
  set xlabel 'Number of Excluded Values';
  set format x '$%.0t \cdot 10^{%T}$';
  set ylabel 'Time (Seconds)';
  set output '$output';
  plot '$tmpfile' using 1:2 with linespoints title 'Without BF',
       '$tmpfile' using 1:3 with linespoints title 'With BF'"

rm $tmpfile
@
As we can see in the results in \cref{fig:acceleration_by_count} we get a \emph{real} win with Bloom filters.
In reality we should also profile the resulting programs and confirm that the execution really changes in the way we expect.
I've seen silly things like one-half of the experiment accidentally allocate everything twice.
Profiling serves the purpose of getting the real ``root cause'' of the performance change.
For this exposition, however, let's just take it on faith.
Sequel work can give instrumented assembly.
\begin{figure}[ht]
    \centering
    \input{acceleration-by-count.fig.tex}
    \caption{Comparing Speedup With Bloom Filters}
    \label{fig:acceleration_by_count}
\end{figure}

We may still be curious in one more variation: recall the Bloom filters interestingly \emph{don't} take up more room based on the size of each \emph{element} (only the count of elements).
Would this be reflected in much-larger strings, so that the [[std::set]] has to do even more, worse, string comparisons?
The following is the same experiment as before, but note [[-l 240]]: our input strings are getting pretty big.
<<acceleration-by-count-big.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
tmpfile=$(mktemp)
for miss_count in {1..10}; do
  count="$miss_count"00000 
  result=$(./driver -a -e 0.01 -n 1000000 -x $count -l 240)
  echo "$count $result"
done > $tmpfile

gnuplot -e "
  set terminal tikz size 5in,2.4in;
  set xlabel 'Number of Excluded Values';
  set format x '$%.0t \cdot 10^{%T}$';
  set ylabel 'Time (Seconds)';
  set output '$output';
  plot '$tmpfile' using 1:2 with linespoints title 'Without BF',
       '$tmpfile' using 1:3 with linespoints title 'With BF'"

rm $tmpfile
@

Surprisingly, \emph{the Bloom filter} slows down in \cref{fig:acceleration_by_count_big}!
Or at least, the wins are smaller.
While contrary (at least superficially) to our theoretical understanding, some ad-hoc profiling I did suggests we're spending a lot of time hashing the strings to compute the Bloom filter bits.
I think there's a lot more insight to mine here, but again for brevity we'll leave more analysis for another exposition.
\begin{figure}[ht]
    \centering
    \input{acceleration-by-count-big.fig.tex}
    \caption{Comparing Speedup with Bloom Filters on Big Strings}
    \label{fig:acceleration_by_count_big}
\end{figure}

We've previously committed to $\epsilon=0.01$. Maybe that's too much.
Focusing just on the Bloom filter case now, how much speedup do we get from $\epsilon=0.01$ versus $\epsilon=0.1$?
We'll not look at those exact numbers, but here is such an experiment:
<<acceleration-by-precision.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
e=0.8
for i in {1..8}; do
  result=$(./driver -a -e $e -n 1000000 -x 1000000 -l 24)
  e=$(echo "$e / 2" | bc -l)
  echo "$e $result"
done | gnuplot -e "
  set terminal tikz size 5in,2.4in;
  set xlabel 'Specified $\epsilon$';
  set xrange [*:1];
  set logscale x;
  set ylabel 'Time (Seconds)';
  set output '$output';
  plot '-' using 1:3 with linespoints title 'With BF'"
@

The results in \cref{fig:acceleration_by_precision} suggests it's worth cranking up the $\epsilon$ to a pretty low number.
\begin{figure}[ht]
    \centering
    \input{acceleration-by-precision.fig.tex}
    \caption{Runtime Variation by Epsilon}
    \label{fig:acceleration_by_precision}
\end{figure}

\section{Parting Thoughts}
Each experiment I ran lead to more thoughts on \emph{other} interesting experiments to run.
That's fun and exciting!
Bloom filters are really amenable to implementation and experimenation.
I wonder if we can get to such a point for other, more sophisticated theoretical objects like those used in SAT solvers.
The ``wins'' in Bloom filters partly come from using uniform random numbers (hashing), and so to a sense it's workload-agnostic: we only have a few dimensions of the input to consider (and the moment we consider another dimension, e.g., input-string-length, we do start getting unintuitive results).
SAT solvers, compilation algorithms, and other ``big'' things seem much more sensitive to workloads.
Just something I'm ruminating on.

I think literate code, at best, comes at the very, very end.
Trying to write this code ``notebook-style'' just didn't work for me.

While the empiricism in this write-up was fun and I think a valuable (uncommon) contribution, it was not as obviously-educational as I would have guessed.
I think for a student this hopefully demonstrates some new ways of thinking about programming in-the-small (a discrete boundary between our library and our driver-program, things like [[getopt]]), and that it's \emph{possible} to place theoretical results in practice.
I would be curious if a students walks away feeling empowered to play with other theoretical objects in code.
Maybe C++ was the wrong choice; I thought it'd help with, e.g., profiling.

\appendix
\section{Library Code Listings}
\subsection{Bloom Filter (bloomfilter.h)}\label{bloomfilterlisting}
\inputminted[fontsize=\small,linenos=true]{c++}{bloomfilter.h}
\subsection{Simple Bit Set (simplebitset.h)}\label{simplebitsetlisting}
\inputminted[fontsize=\small,linenos=true]{c++}{simplebitset.h}
\section{Driver Code Listings}\label{sec:driver-complete}
\inputminted[fontsize=\small,linenos=true]{c++}{driver.cpp}
\end{document}

% Original paper:
% https://dl.acm.org/doi/10.1145/362686.362692
% https://dl.acm.org/doi/pdf/10.1145/362686.362692

