\documentclass{article}
\author{Aaron Gorenstein}
\title{Bloom Filter Implementation}
\date{\today}

\usepackage{hyperref}
\usepackage{noweb}
\usepackage{cleveref}
\usepackage{listing}
\usepackage{minted}

\usepackage{biblatex}
\addbibresource{library.bib}

\begin{document}
\maketitle

\tableofcontents

\section{Introduction}
As a data structure to explore, Bloom filters\cite{bloom1970} seem to hit the ``sweet spot'' in a number of ways.
\begin{enumerate}
\item They're practical, and used in industry.
\item They have many interesting extensions, e.g., 
\item They are amenable to analysis, and indeed are literally a textbook example of some probabilistic technique-applications\cite{mitzenmacher2005probability}.
\end{enumerate}
Moreover, as we'll see, they \emph{readily} translate from an abstract, analysis-focused description
to a (serviceable) real-world example.

Most educational treatments of Bloom filters naturally focus on the mathematical analysis of their properties.
Here we'll try to bridge the gap between theoretical treatments of Bloom filters and how they're realized in code.
I would expect in this case, as in other topics, there is a still-further jump from a demonstrative implementation (as we'll develop here) and an ``industrial-strength'' one,
so this is not the end of the story

Theoretical treatments typically present their code with psuedocode, which is understandable: it makes it easier to focus on the qualities that dictate the formal analysis.
The gap we will try to jump is not ``just'' psuedocode.
Motivation for any data structure or algorithm comes from its applications, and I have personally seen most treatments be content with a few prose sentences.
This is to those treatments' detriments, in my opinion.
Seeing a full, running example of (in this case) how a Bloom filter can be used to speed up set membership tests, that helped build
conceptual ``hooks'' in my mind on which I could attach the more fundamental concepts of them.
I will say more about this as we get there in \cref{sec:application}.

The rest of this document will follow this structure:
\begin{itemize}
\item In \cref{sec:theorytocode} we'll present our demonstrative C++ implementation of a bloom filter.
The one implementation detail we will \emph{not} drill into is the hashing scheme; we need a collection $h_i$ of hash fucntions,
and we'll do so using MurmurHash3.
\item In \cref{sec:application} we'll build a Bloom-filter-enhanced container in C++, and list out some expectations of what we'd if the Bloom filter helps us as we'd want.
\item In OTHER SECTION we'll \emph{apply} the analytical results
\end{itemize}
List out each section and what we'll get out of them.
One thing intentionally missing from this treatment is an introductory description of what a Bloom filter \emph{is}.
For that I can only recommend online resources or books\cite{mitzenmacher2005probability}.

% ++++++++++++++++++++++++
% 
% Bloom filters seem to hit just the ``sweet spot'' for interesting data structures.
% They were introduced decades ago\cite{bloom1970}, and are a practical data structure, still in use today.
% They are interesting: they are a probabilistic data structure, and exhibit a curious behavior (false-positives) that mean they only have certain applications.
% Their expected performance is amenable to analysis, but not trivial (at least for me).
% They are extensible; a great many variaties exist.
% Lastly, they are simple, in the sense that they can be easily implemented, and consequently explored empirically.
% 
% It is all of these, but especially the last part on implementation, that motivates this work.
% Here we will implement, using only standard C++, a basic Bloom filter. This includes:
% \begin{enumerate}
% \item Its core implementation and helper objects
% \item Experiments validating its behavior as predicted by mathematical analysis
% \item Experiments validating its expected benefit in a demonstrative application
% \end{enumerate}
% What we will \emph{not} do is introduce what a Bloom filter is, we trust you're already familiar.
% We will also \emph{not} include any analysis of the hashing algorithm we will use.
% Finally, the above-mentioned experiments will also be suitably humble in scope.

% The expected benefit of this work is to equip others to ``follow along'' and take the code further,
% and moreover to show that we can---and should more often!---jump the bridge between theory and implementation.
% Without further ado we can jump into implementing Bloom filters.
% 
% Let's explore how to map a general description, in this case the one from a standard textbook\cite{mitzenmacher2005probability},
% and see how it maps to code.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bloom Filter: Theory to Code}\label{sec:theorytocode}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We will move from 
As described in our reference, we want a length-$n$ sequence $A$ of bits.
We additionally specify $k$ hash-functions.
\subsection{Bloom Filter Implementation}
<<Bloom filter state and construction>>=
private:
  simple_bit_set A;

public:
  const size_t n;
  const size_t k;
  bloom_filter(const size_t n, const size_t k) : A(n), n(n), k(k) {}
@
The field [[A]] is the core sequence of bits.
When we talk about ``setting'' or ``checking'' bits, we are referencing [[A]].
The parameter [[n]] is the number of bits we'll be using in our sequence.
Finally, [[k]] is the number of bits we set for each value.

As an implementation detail, the type of our container, as well as its hashing scheme,
are passed as \emph{template} parameters.
<<Bloom Filter C++ Type Interface>>=
template <class V, size_t (*h)(uint32_t i, const V &)>
@
Given the C++ type system and its model of containers in its standard library,
this seems like the appropriate way to specify this.
Here already we have something of an extra hurdle from theoretical descriptions: we have to make sure our implementation plays nicely with our programming language's type system!

With our parameters, and how they're set in our object, defined, we can turn to the key functionality:
how do we add elements to our filter, and how do we test if an element was added to our filter?
For these two operations we'll map the theoretical prose into code.
These are all from \cite{mitzenmacher2005probability}.

\paragraph{Adding Elements}
\begin{quote}
For each element $s\in S$, the bits $A[h_i(s)]$ are set to 1 for $1\leq i \leq k$.
A bit location can be set to 1 multiple times, but only the first change has an effect.
\end{quote}
<<Bloom filter set>>=
void set(const V &s) {
  for (size_t i = 0; i < k; i++) {
    A.set(h(i, s) % n);
  }
}
@

\paragraph{Testing an Element}
\begin{quote}
To check if an element $x$ is in $S$, we check whether all array locations $A[h_i(x)]$
for $1 \leq i \leq k$ are set to 1. If not, then clearly $x$ is not a member of $S$,\ldots.
If all $A[h_i(x)]$ are set to 1, we assume $x$ is in $S$, although we could be wrong.
\end{quote}
<<Bloom filter test>>=
bool test(const V &s) const {
  for (size_t i = 0; i < k; i++) {
    if (!A.test(h(i, s) % n)) {
      return false;
    }
  }
  return true;
}
@
Note on application: the filter is distinct from the set!
We're inclined to model the bloom filter as an object, as we do here, but the psuedocode just talks about $A$.

\paragraph{Tying it together}
This completes the core of our implementation.
We can wrap these up in a stand-alone C++ header defining this object, like so:
<<bloomfilter.h>>=
#pragma once
#include "simplebitset.h"
#include "murmur/MurmurHash3.h"
#include <cstddef>
#include <cstdint>
#include <string>
<<Bloom Filter C++ Type Interface>>
class bloom_filter {
  <<Bloom filter state and construction>>
  <<Bloom filter set>>
  <<Bloom filter test>>
  int count() const { return A.count(); }
};

<<Hashing Function>>
@

We are not quite done with our implementation.
Two things remain:
\begin{enumerate}
\item Implementing our array-of-bits [[simple_bit_set]]
\item Providing concrete arguments for the template parameters [[V]] and [[h]].
\end{enumerate}

We will address these in turn.

\subsection{Array of Bits}
Some languages have this built in already.
The language C++ has it, but not really, so we do this:
<<simplebitset.h>>=
#include <bitset>
#include <numeric>
#include <vector>
class simple_bit_set {
private:
  static const int pageSize = 64;
  typedef std::bitset<pageSize> page;
  std::vector<page> data;

public:
  simple_bit_set(const int size) : data((size / pageSize) + 1, 0) {}
  bool test(unsigned int i) const {
    return data[(i / pageSize)].test(i % pageSize);
  }
  void set(unsigned int i) { data[(i / pageSize)].set(i % pageSize); }
  int count() const {
    int r = 0;
    for (const auto& p : data) { r += p.count(); }
    return r;
  }
  int size() const { return data.size() * pageSize; }
};
@

\subsection{Hashing Scheme}
We use murmur3!
Something I'm interested in: a nice \emph{theory} of a hashing algorithm.
There are great empirical results that this is a good hashing algorithm.
What explains that performance?

<<Hashing Function>>=
size_t string_hash(uint32_t i, const std::string &v) {
  uint32_t hash;
  MurmurHash3_x86_32(v.c_str(), v.size(), i, &hash);
  return static_cast<size_t>(hash);
}
@

\section{Bloom Filter Application}\label{sec:application}
At this point, we've essentially emulated the typical introduction to a Bloom filter\cite{mitzenmacher2005probability}.


Let's demonstrate 
<<bloomfilterset.h>>=
#include "bloomfilter.h"
#include <set>
template <class V, size_t (*F)(uint32_t, const V &)>
class bloom_filtered_set {
  std::set<V> core_data;
  bloom_filter<V, F> filter;

public:
  bloom_filtered_set(size_t size, size_t k) : filter(size, k) {}
  void add(const V &v) {
    filter.set(v);
    core_data.insert(v);
  }
  int contains(const V &v) const {
    if (!filter.test(v)) { return false; }
    return core_data.find(v) != core_data.end();
  }
};
@

\section{Using Analytical Results}
Most treatments\cite{mitzenmacher2005probability,bloom1970} spend time on how to derive the analytical results.
This document is meant to complement those by providing some empirical demonstrations.
We will not spend time deriving the analytical results, but will instead ``just'' demonstrate how to plug in those results to use a Bloom filter effectively.
The analytical results inform the parameters $k$ and $n$, basically.

When we use a bloom filter, we care about the \emph{size} of the filter, and its \emph{false-positive} rate.
The size is ``just'' the size of the explicit object, the same $n$ that says how big [[simple_bit_set]] is.
The false-positive rate is more dynamic: it is the ratio of EXPRESS IN TERMS OF CODE, [[filter_hit]] AND SUCH.

We can set $n$ directly, of course, and the other parameter we have is $k$.
We also want an estimate of the \emph{final} size of our set $m$.
(This $m$ is the number of elements that ultimately end up in [[core_data]].)
This introduction of $m$ implies something subtle: many times in code we use set-data-structures to add and remove elements frequently.
A Bloom filter is geared towards summarizing a \emph{read-only-set}---the collection of data we care about already exists and is fixed, and we do a one-time generation.
DRILL INTO THIS MORE, MAYBE IN ANOTHER SECTION (EARLIER?). DIFFERENCE BETWEEN THEORY AND PRACTICE, THEY LEAVE THESE APPLICATION RESTRICTIONS IMPICIT.

So in that spirit, let's fix $m$. As we'll
\section{Empirical Validation}\label{sec:empvalidation}
\subsection{Creating Data}
We need resources by which we can run our experiments.
In this case, we need a large set of strings to populate our bloom filter,
and then a collection of strings that may (or may not) be in the set.
We can find real-world datasets, but for our purposes we can ``just'' generate random strings.

Let's model our need for random strings as a generator:
<<String Generator Class>>=
class string_generator {
  std::random_device rd;
  std::default_random_engine e1;
  std::uniform_int_distribution<int> uniform_dist;
public:
  string_generator();
  std::string operator()(size_t len);
};
@

There are a lot of particulars about the [[std]] machinery used here that we won't get in to.
For our purposes, we're just using them as a ``black box'' to get a nice selection of random strings expediently.

Explicitly, we can get random strings like:
<<Generator Implementation>>=
const static std::string letter_set =
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";

string_generator::string_generator()
    : e1(rd()), uniform_dist(0, letter_set.size()) {}

std::string string_generator::operator()(size_t len) {
  std::string next;
  next.reserve(len);
  for (size_t c = 0; c < len; c++) {
    next += letter_set[uniform_dist(e1)];
  }
  return next;
}
@

The [[letter_set]] collection is just a way to, in case if it's needed, ensure our
random strings are easily human-readable.
The constructor essentially handles the random-number-generator-initialization.
Finally, the [[operator()]] method is how we get our string.

How is this generator used?
Our intention is to create a collection of random strings.
For expedience and conceptual simplicity, what we'll do is use our generator as much as we
need to generate random strings, and add them to a set.
In this way, when our set (which will implicitly exclude any duplicates!) will end up
with the number of strings we asked for.
<<Creating Strings with the Generator>>=
std::vector<std::string> create_strings(size_t count, size_t len) {
  std::unordered_set<std::string> unique_strings;
  string_generator g;
  while (unique_strings.size() < count) {
    unique_strings.insert(g(len));
  }
  std::vector<std::string> data(std::begin(unique_strings),
                                std::end(unique_strings));
  return data;
}
@

So we have our generator package all set up:

<<datagen.h>>=
#include <random>
#include <string>
#include <vector>
<<String Generator Class>>
std::vector<std::string> create_strings(size_t count, size_t len);
@

<<datagen.cpp>>=
#include "datagen.h"

#include <unordered_set>
<<Generator Implementation>>
<<Creating Strings with the Generator>>
@

\subsection{Experiment Driver}
We can understand our previous code (\cref{bloomfilter.h, bloomfilterset.h}) as creating a library.
In general one does not really ``run'' a library; it only makes sense to run the code as part of a larger piece of software.
This is reflecting mechanically in our C++ code by the fact that none of our code so far has a [[main]] function.

A common piece of software to provide with a library is a \emph{driver}, which can be understood as a limited piece of software,
typically a command-line utility, that can take user input and exercise different pieces of code in the library.
Code-compilers typically operate in this way, most notably LLVM makes that very explicit \cite{TOCITE}, but also other processing libraries.

The code we'll implement for our experiments will not quite be an all-purpose driver, but is influenced by that mindset.
The overall design is outlined in \cref{driver.cpp}.

<<driver.cpp>>=
<<Driver Headers>>
<<Driver Utility Functions>>
<<Bloom Filter Settings>>
<<Possible Bloom Filter Experiments>>
int main(int argc, char* argv[]) {
  <<Read Command-Line Parameters>>
  <<Error-Check Parameters>>
  <<Prepare Experiments>>
  <<Run Selected Experiments>>
}
@

Here are all the settings we'd need for any experiment:
<<Bloom Filter Settings>>=
double desired_epsilon = 0.0;
int num_strings_in_set = 0;
int num_strings_not_in_set = 0;
int length_of_strings = 0;
  
bool print_help = false;
int verbosity_level = 0;

size_t m = 0;
size_t k = 0;
  
bool run_density_experiment = false;
bool run_false_positive_experiment = false;
bool run_application_experiment = false;
@
We set those settings through command-line parsing:
<<Read Command-Line Parameters>>=
int c;
while ((c = getopt(argc, argv, "dfae:n:m:l:hv:")) != -1) {
  switch (c) {
    // Which experiments to run:
    case 'd': run_density_experiment = true; break;
    case 'f': run_false_positive_experiment = true; break;
    case 'a': run_application_experiment = true; break;
    
    // Parameters to the experiment
    case 'e': desired_epsilon = std::strtod(optarg, nullptr); break;
    case 'n': num_strings_in_set = std::strtol(optarg, nullptr, 0); break;
    case 'm': num_strings_not_in_set = std::strtol(optarg, nullptr, 0); break;
    case 'l': length_of_strings = std::strtol(optarg, nullptr, 0); break;

    // Meta-parameters
    case 'h': print_help = true; break;
    case 'v': verbosity_level = std::strtol(optarg, nullptr, 0); break;
  }
}
@
We want to make sure that we aren't asking for anything silly.
There are probably more problems we can find, but this helps address simple typos
<<Error-Check Parameters>>=
if (!run_density_experiment && !run_false_positive_experiment && !run_application_experiment) {
  printf("Error: no requested experiment!\n");
  return 1;
}
if (desired_epsilon < 0 || desired_epsilon >= 1) {
  printf("Error: desired_epsilon must be between 0 and 1, got %f\n", desired_epsilon);
  return 1;
}
if (num_strings_in_set < 0) {
  printf("Error: num_strings_in_set must be non-negative, got %d\n", num_strings_in_set);
  return 1;
}
if (length_of_strings <= 0) {
  printf("Error: length_of_strings must be positive, got %d\n", length_of_strings);
  return 1;
}
@
We prepare experiments by calculating m and k if they're not specified
<<Prepare Experiments>>=
<<Print Driver Parameters>>
const double best_bits = -1.44 * std::log2(desired_epsilon);
m = num_strings_in_set * best_bits;
k = size_t(-std::log2(desired_epsilon)) + 1;
if (verbosity_level >= 1) {
  printf("Calculated best bits per value: %f, total number of bits: %ld, number of hash functions: %ld\n",
           best_bits, m, k);
}
@
And then finally we run the experiments we choose:
<<Run Selected Experiments>>=
if (run_density_experiment) {
  optimal_density();
}
if (run_false_positive_experiment) {
  if (num_strings_not_in_set <= 0) {
    printf("Error, for false positive experiment num_strings_not_in_set must be positive, got %d\n",
           num_strings_not_in_set);
    return 1;
  }
  false_positive_rate();
}
if (run_application_experiment) {
  if (num_strings_not_in_set <= 0) {
    printf("Error, for timing experiment num_strings_not_in_set must be positive, got %d\n",
           num_strings_not_in_set);
    return 1;
  }
  bloom_filter_application();
}
@

<<Possible Bloom Filter Experiments>>=
<<Bloom Filter False Positive Rate>>
<<Bloom Filter Optimal Density>>
<<Bloom Filter Application>>
@

<<Driver Utility Functions>>=
@

<<Driver Headers>>=
#include <algorithm>
#include <unistd.h>
#include <cstdlib>
#include <cstdio>
#include <cmath>
#include <chrono>
#include <set>

#include <iostream>

#include "bloomfilterset.h"
#include "datagen.h"
@

<<Print Driver Parameters>>=
if (verbosity_level != 0) {
  printf("parameters:\n"
    "\tprint_help=%s\n"
    "\tverbosity_level=%d\n"
    "\trun_density_experiment=%s\n"
    "\trun_false_positive_experiment=%s\n"
    "\trun_application_experiment=%s\n"
    "\tdesired_epsilon=%f\n"
    "\tnum_strings_in_set=%d\n"
    "\tnum_strings_not_in_set=%d\n"
    "\tlength_of_strings=%d\n",
    print_help ? "true" : "false", verbosity_level,
    run_density_experiment ? "true" : "false",
    run_false_positive_experiment ? "true" : "false",
    run_application_experiment ? "true" : "false",
    desired_epsilon, num_strings_in_set, num_strings_not_in_set,
    length_of_strings);
}
@

At this point, we have our analytical results, our Bloom filter implementation,
and a means to exercise that Bloom filter.
Let's get some empirical results!

\subsection{Compute Expected Density of Optimal Bloom Filter}
<<Bloom Filter Optimal Density>>=
void optimal_density(void) {
  const auto strings = create_strings(num_strings_in_set, length_of_strings);
  if (verbosity_level >= 2) { printf("Done generating strings\n"); }

  bloom_filter<std::string, string_hash> bf(m, k);
  for (auto& s : strings) {
    bf.set(s);
  }
  const double density =
    static_cast<double>(bf.count()) / static_cast<double>(bf.n);
  printf("bloom filter density: %f\n", density);
}
@


\subsection{Bloom Filter False Positivity}
This is one experiment!
<<Bloom Filter False Positive Rate>>=
void false_positive_rate(void) {
  auto total_strings = num_strings_in_set + num_strings_not_in_set;
  if (verbosity_level >= 1) {
    printf("total_strings: %d\n", total_strings);
  }
  const auto strings = create_strings(total_strings, length_of_strings);
  if (verbosity_level >= 2) { printf("Done generating strings\n"); }
  auto strings_in_set_end = std::begin(strings) + num_strings_in_set;

  bloom_filter<std::string, string_hash> bf(m, k);
  std::for_each(std::begin(strings), strings_in_set_end,
                [&bf](auto &s) { bf.set(s); });

  auto false_positives = std::count_if(strings_in_set_end, std::end(strings),
                                       [&bf](auto &v) { return bf.test(v); });
  double fp_rate = double(false_positives) /
                   double(num_strings_not_in_set);
  printf("false positive rate: %f\n", fp_rate);
}
@

\subsection{Accelerating A Set}

<<Bloom Filter Application>>=
void bloom_filter_application(void) {
  auto total_strings = num_strings_in_set + num_strings_not_in_set;
  if (verbosity_level >= 1) {
    printf("total_strings: %d\n", total_strings);
  }
  const auto strings = create_strings(total_strings, length_of_strings);
  if (verbosity_level >= 2) { printf("Done generating strings\n"); }
  auto strings_in_set_end = std::begin(strings) + num_strings_in_set;
  
  std::set<std::string> base_set;
  bloom_filtered_set<std::string, string_hash> filtered_set(m, k);
  for (auto it = std::begin(strings); it != strings_in_set_end; it++) {
    base_set.insert(*it);
    filtered_set.add(*it);
  }

  auto to_test_begin = strings_in_set_end;
  auto to_test_end = std::end(strings);
  {
    auto start = std::chrono::steady_clock::now();
    size_t count = 0;
    for (auto it = to_test_begin; it != to_test_end; it++) {
      if (base_set.find(*it) != base_set.end()) {
        count++;
      }
    }
    auto end = std::chrono::steady_clock::now();
    
    std::chrono::duration<double> elapsed = end - start;
    printf("Tested against base in time %fs\n", elapsed.count());
  }
  {
    auto start = std::chrono::steady_clock::now();
    size_t count = 0;
    for (auto it = to_test_begin; it != to_test_end; it++) {
      if (filtered_set.contains(*it)) {
        count++;
      }
    }
    auto end = std::chrono::steady_clock::now();

    std::chrono::duration<double> elapsed = end - start;
    printf("Tested against diff in time %fs\n", elapsed.count());
  }
}
@
We validate the filter-hit rate.
We also do timing.
The goal of this experiment is to build some confidence that a Bloom filter provides
a benefit.


\section{Conclusion}
Isn't this neat.
\appendix
\section{Complete Code Listings}
\subsection{Bloom Filter (bloomfilter.h)}\label{bloomfilterlisting}
\inputminted{c++}{bloomfilter.h}
\subsection{Bloom Filter (bloomfilter.h)}\label{bloomfiltersetlisting}
\inputminted{c++}{bloomfilterset.h}
\end{document}

% Original paper:
% https://dl.acm.org/doi/10.1145/362686.362692
% https://dl.acm.org/doi/pdf/10.1145/362686.362692