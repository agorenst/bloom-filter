\documentclass{article}
\author{Aaron Gorenstein}
\title{Bloom Filter Implementation}
\date{\today}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{noweb}
\usepackage{cleveref}
\usepackage{listing}
\usepackage{minted}
\usepackage{tikz}
% thanks: http://gnuplot.info/docs/latex_demo.pdf
\usepackage{gnuplot-lua-tikz}

\usepackage{biblatex}
\addbibresource{library.bib}

\begin{document}
\maketitle

\tableofcontents

\section{Introduction}
Formal computer science---including things like algorithms and data structures, computational complexity theory,
as well as the more ``formal'' side of practical things like compilers, databases, and operating systems---is all just so fascinating.
In an academic settings, i.e., textbooks and college courses, treatments of these topics typically focus on the underlying ideas to the exclusion of practical implementations.
This is understandable: the \emph{insights} of computer science are independent of the programming language, operating system, or hardware (even as some of those insights greatly influence these things!),
and there are only so many pages in a textbook, or lectures in a course.

I think, however, such treatments would benefit from practical implementations of their ideas.
This paper is the first in a hoped-for series that provides a stand-alone\footnote{In the sense that only a standard compiler and \emph{minimal} external dependencies are needed.},
practical\footnote{In the sense that the code runs and, at least sometimes, exhibits its purported behavior.}, and approachable\footnote{In the sense that, with the accompanying text, readers will find the code completely comprehensible.}
implementation of a ``textbook'' algorithm.

Herein, we will discuss Bloom filters, introduced\cite{bloom1970} over 50 years ago and still going strong.
From the perspective that this document is to \emph{supplement} existing textbook discussions (usually in specialized algorithms textbooks\cite{mitzenmacher2005probability}),
we will jump as quickly as possible into a concrete implementation.
The rest of the document is as follows:
\begin{itemize}
\item In \cref{sec:theorytocode} we will provide a complete implementation of a Bloom filter in standard C++17.
\item In \cref{sec:harnesscode} we will provide a stand-alone ``driver'' program that can take command-line parameters and actually execute code using the Bloom filter, so that we might
examine the behavior of our implementation in a real environment.
\item Finally, in \cref{sec:experiments}, we will present some findings and thoughts based on using that driver.
\end{itemize}
Along the way we will have brief interludes discussing how our practical work and findings complement academic treatments, for the learner.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bloom Filter: Theory to Code}\label{sec:theorytocode}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This section ``reads along'' with a textbook description of Bloom filters\cite{mitzenmacher2005probability},
and derives code from the prose.

To begin, as described in our reference, we want a length-$n$ sequence $A$ of bits.
We additionally specify $k$ hash-functions.
This is the entirety of the state of our Bloom filter, so we can write:
\subsection{Bloom Filter Implementation}
<<Bloom filter state and construction>>=
private:
  simple_bit_set A;

public:
  const size_t n;
  const size_t k;
  bloom_filter(const size_t n, const size_t k) : A(n), n(n), k(k) {}
@
The field [[A]] is the core sequence of bits.
When we talk about ``setting'' or ``checking'' bits, we are referencing [[A]].
The parameter [[n]] is the number of bits we'll be using in our sequence.
Finally, [[k]] is the number of bits we set for each value we insert, i.e., the number of hash functions we need.

As an implementation detail, the type of our container, as well as its hashing scheme,
are passed as \emph{template} parameters.
<<Bloom Filter C++ Type Interface>>=
template <class V, size_t (*h)(uint32_t i, const V &)>
@
Given the C++ type system and its model of containers in its standard library,
this seems like the appropriate way to specify this.

\paragraph{Interlude: Theory To Practice}
What are some more ways we might specify the hash functions for our Bloom filter?
Observe that our implementation very directly maps to the prose: we expect the hash function to look something like [[h(i, v)]], where [[v]] is the value we want to hash, and [[i]] indicates that we should use the $i$-th hash function.
An alternative implementation might be one where we have a single hash function, and the Bloom filter extends that to create multiple hash functions.
Additionally, we're using a very C++-specific feature of \emph{templates}.
The effect is that the function-pointer provided is going to be a compile-time constant, sparing us a likely-very-expensive indirect call.
An alternative would be to provide the hashing function at runtime, which (modulo very sophisticated compiler optimizations) would indeed induce an indirect call.
A separate practical consideration is our negotiating with our language's type-system, another detail frequently (understandably!) elided in typical academic introductions.

With our parameters, and how they're set in our object, defined, we can turn to the key functionality:
How do we add elements to our filter, and how do we test if an element was added to our filter?
For both operations we will consider a quote from our reference, and then map that to code.
Happily, at this point we will find the mapping very straightforward.

\paragraph{Adding Elements}
\begin{quote}
For each element $s\in S$, the bits $A[h_i(s)]$ are set to 1 for $1\leq i \leq k$.
A bit location can be set to 1 multiple times, but only the first change has an effect.\cite[109]{mitzenmacher2005probability}.
\end{quote}
<<Bloom filter set>>=
void set(const V &s) {
  for (size_t i = 0; i < k; i++) {
    A.set(h(i, s) % n);
  }
}
@

\paragraph{Testing an Element}
\begin{quote}
To check if an element $x$ is in $S$, we check whether all array locations $A[h_i(x)]$
for $1 \leq i \leq k$ are set to 1. If not, then clearly $x$ is not a member of $S$,\ldots.
If all $A[h_i(x)]$ are set to 1, we assume $x$ is in $S$, although we could be wrong.\cite[109]{mitzenmacher2005probability}.
\end{quote}
<<Bloom filter test>>=
bool test(const V &s) const {
  for (size_t i = 0; i < k; i++) {
    if (!A.test(h(i, s) % n)) {
      return false;
    }
  }
  return true;
}
@

For our demonstrative implementation, that is all we need! This completes the core of our implementation.
We can wrap these up in a stand-alone C++ header defining this object, like so:
<<bloomfilter.h>>=
#pragma once
#include "simplebitset.h"
#include <cstddef>
#include <cstdint>
<<Bloom Filter C++ Type Interface>>
class bloom_filter {
  <<Bloom filter state and construction>>
  <<Bloom filter set>>
  <<Bloom filter test>>
  int count() const { return A.count(); }
  size_t size() const { return A.size(); }
};
@
(We haven't mentioned the [[count]] method written above. That counts the number of bits-set-to-1, i.e., some value $c \leq n$, and will be used in later experiments.)

There are still a few loose ends.
Two things remain:
\begin{enumerate}
\item Implementing our array-of-bits [[simple_bit_set]]
\item Providing concrete arguments for the template parameters [[V]] and [[h]].
\end{enumerate}

We will address these in turn.

\subsection{Array of Bits}
Some languages have this built in already.
The standard C++ libraries provide a \emph{fixed-length} (at compile time) bit-array, but combining it with the dynamically-resizable [[std::vector]] container leads to a natural implementation:
<<simplebitset.h>>=
#pragma once
#include <bitset>
#include <numeric>
#include <vector>
class simple_bit_set {
private:
  static const int page_size = 64;
  typedef std::bitset<page_size> page;
  std::vector<page> data;

public:
  simple_bit_set(const int size) : data((size / page_size) + 1, 0) {}
  bool test(unsigned int i) const {
    return data[(i / page_size)].test(i % page_size);
  }
  void set(unsigned int i) {
    data[(i / page_size)].set(i % page_size);
  }
  int size() const { return data.size() * page_size; }
  int count() const {
    int r = 0;
    for (const auto& p : data) { r += p.count(); }
    return r;
  }
};
@
The one ``trick'' that may be new to implementors is how we ``translate'' [[i]] first into which 64-bit [[page]] to look up, and then look up the right bit within that page.
We can also drill into another detail: the number of bits we set will always be some multiple of [[page_size]].
Consequently, we may sometimes have slightly more bits than our Bloom filter expects.
This isn't anticipated to appreciably change the performance or even the analysis, but another example of a ``rough edge'' between theory and practice.

\subsection{Hashing Scheme and Values to Hash}
We need a so-called ``good'' hash for our Bloom filter.
Hashing algorithms are a whole field into themselves, and one I hope to explore later.
For now, we will defer to what I hear is established as ``good practice'' and use the public-domain MurmurHash3 algorithm\cite{murmur3}.
This is understood to be an extremely fast and good hash, without the overhead of cryptographic guarantees.
It provides a simple C-header interface, and most conveniently provides a ``seed'' parameter that exactly matches the use of $i$ in $h_i$.

Here is our explicit hashing function:
<<Hashing Function>>=
size_t string_hash(uint32_t i, const std::string &v) {
  uint32_t hash;
  MurmurHash3_x86_32(v.c_str(), v.size(), i, &hash);
  return static_cast<size_t>(hash);
}
@
Note that implicit in our implementation is that we've also picked our type [[V]]: we'll be hashing a collection of [[std::string]]s.
For our type system, generally, we'd want to have a hash function implemented ``in tandem'' with our value-type.
This is so that it knows how to look at the whole value.
In this case, a [[std::string]] value is actually a small object holding data like the length, and a \emph{pointer} to the ``real'' content of the string.\footnote{
  In practice things are actually even a bit more complicated for [[std::string]], where sometimes due to the so-called ``short-string optimization'' there isn't a pointer in use!
}
We should hash the \emph{contents} of the pointer rather than the \emph{value} of the pointer.
To hash the values would be a bug:
In the case where we have two copies of the same string, their underlying bytes may exist in different memory locations so a naive hash would say they're \emph{different}, but we need them to be reported the same!
So our [[string_hash]] implementation takes care of telling our underlying hash algorithm which data to look at, and how, for hashing.

\paragraph{Interlude: Practical Considerations in Code}
Domain and casting, TODO, write about that.

\paragraph{Personal Interlude} More than once in my career a colleague has said ``we need a hashing algorithm that provides good guarantees. Let's use SHA!''
In a sense, I agree: the properties promised by \emph{cryptographic} hashing functions are very nice.
My background in computational complexity reminds me always that features (usually) have a price---in this case, CPU time.
When we just want to get a nice hashing algorithm for some in-memory object, we typically don't need to worry about hackers trying to, given the hash result, figure out the actual value!

This completes all the components of our implementation.

\section{Harnessing the Code}\label{sec:harnesscode}
Now we have a complete implementation of a Bloom filter.
We expect to be able to use it in C++ with code like [[bloom_filter<std::string, string_hash> bf(m, k)]].
That creates a Bloom filter [[bf]] to which we can add [[std::string]]-typed values, and later test if they were (likely) included.
Our goal is to actually exercise this code. For that, we need:
\begin{enumerate}
\item A way to find or create strings to add to the Bloom filter.
\item A way to find or create (possibly different) strings to test-membership-of in the Bloom filter.
\item A way to observe expected properties of a Bloom filter: namely its false-positive rate and its bit-density.
\item A way to compare the application of a Bloom filter and see if its use saves time.
\end{enumerate}
And possibly more!

We will find that writing the supporting code for this application is maybe not as conceptually sophisticated as the \emph{ideas} behind a Bloom filter,
but is nonetheless nontrivial and affords opportunities for good---and bad---design.

In the following subsections we'll fill out a piece of software, a \emph{driver}, that provides a command-line interface to run various Bloom filter experiments.

\subsection{Creating Data}
First, we need strings.
We can find real-world datasets, but for our purposes we can ``just'' generate random strings.

Let's model our need for random strings as a generator:
<<String Generator Class>>=
class string_generator {
  std::random_device rd;
  std::default_random_engine e1;
  std::uniform_int_distribution<int> uniform_dist;
public:
  string_generator();
  std::string operator()(size_t len);
};
@

Random number generators are yet-again another field into themselves that again I hope to explore later.
In keeping with our theme, there are a lot of particulars mapping the concepts of RNG (random number generation)
to C++ code, but for this writeup we won't get into them.
For our purposes, we're just using them as a ``black box'' to get a nice selection of random strings expediently.

Explicitly, we can get random strings like:
<<Generator Implementation>>=
const static std::string letter_set =
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";

string_generator::string_generator()
    : e1(rd()), uniform_dist(0, letter_set.size()) {}

std::string string_generator::operator()(size_t len) {
  std::string next;
  next.reserve(len);
  for (size_t c = 0; c < len; c++) {
    next += letter_set[uniform_dist(e1)];
  }
  return next;
}
@

The [[letter_set]] collection is just a way to, in case if it's needed, ensure our
random strings are easily human-readable.
The constructor essentially handles the random-number-generator-initialization.
Finally, the [[operator()]] method is how we get our string: we give it a [[len]] parameter, and it generates a string with that many characters drawn uniformly at random from our letter-set.

The following code and explanation shows how we use this underlying generator:
<<Creating Strings with the Generator>>=
std::vector<std::string> create_strings(size_t count, size_t len) {
  std::unordered_set<std::string> unique_strings;
  string_generator g;
  while (unique_strings.size() < count) {
    unique_strings.insert(g(len));
  }
  std::vector<std::string> data(std::begin(unique_strings),
                                std::end(unique_strings));
  return data;
}
@
Our intention is to create a collection of random strings, and moreover \emph{distinct} random strings.
An expedient way to guarantee distinctness is to add generated strings to a set until the size of the set is the desired amount.
So long as we know there are enough possible strings (i.e., that [[len]] is big enough relative to [[count]]) we'll succeed.
In our case, this will be fast-enough, though I'm sure there are much better ways of doing this.

We can wrap these code samples up into a small software package to link into our driver.
<<datagen.h>>=
#include <random>
#include <string>
#include <vector>
<<String Generator Class>>
std::vector<std::string> create_strings(size_t count, size_t len);
@

<<datagen.cpp>>=
#include "datagen.h"

#include <unordered_set>
<<Generator Implementation>>
<<Creating Strings with the Generator>>
@

\subsection{Experiment Driver}
Recall that we can understand our previous code (\cref{bloomfilter.h}) as creating a library.
In general one does not really ``run'' a library; it only makes sense to run the code as part of a larger piece of software.
This is reflecting mechanically in our C++ code by the fact that none of our code so far has a [[main]] function.

A common piece of software to provide with a library is a \emph{driver}, which can be understood as a limited piece of software,
typically a command-line utility, that can take user input and exercise different pieces of code in the library.
Code-compilers typically operate in this way, most notably LLVM makes that very explicit \cite{TOCITE}, but also other processing libraries.

The code we'll implement for our experiments will not quite be an all-purpose driver, but is influenced by that mindset.
The overall design is outlined in \cref{driver.cpp}.
<<driver.cpp>>=
<<Driver Headers>>
<<Bloom Filter Settings>>
<<Driver Utility Functions>>
<<Possible Bloom Filter Experiments>>
int main(int argc, char* argv[]) {
  <<Read Command-Line Parameters>>
  <<Error-Check Parameters>>
  <<Prepare Experiments>>
  <<Run Selected Experiments>>
}
@
\paragraph{High-Level Driver Design}
From an executable perspective, our driver will read first commmand-line parameters (\cref{Read Command-Line Parameters}),
and then use those to set our experiment-state (\cref{Bloom Filter Settings}).
Some of those parameters set which experiments we'd like to run.
Those experiments are defined in \cref{Possible Bloom Filter Experiments}, and (after error-checking our parameters in \cref{Error-Check Parameters}),
the driver ``just'' runs those experiments and terminates.

In the following sections we'll describe each of our desired experiments.
As we build the understanding of what experiments we'd like to run, that understanding will inform the settings we need, and then the parsing and error-checking.
In that way we'll ``work backwards'' from our goal (run interesting experiments), and only at the end get into the implementation detail of the driver.
We'll actually run the experiments in \cref{sec:experiments}.

\subsection{Compute Expected Density of Optimal Bloom Filter}
As mentioned in our motivating textbook\cite[111]{mitzenmacher2005probability}, an interesting property of an ``optimal'' Bloom filter
is that its density (the proportion of bits set to $1$) should approach $0.5$.
This touches on the complicated question of what is ``optimal'', but for our purposes:
Given a size $N$ collection of strings to hash and desired false-positive rate $\epsilon$, we should be able to determine a ``best'' number-of-bits $m$ and number-of-hashes $k$.
This optimality will be in part reflected by that density.

So let's enable our driver to compute the density of a Bloom filter!
Here's the code:
<<Bloom Filter Optimal Density>>=
void optimal_density() {
  const auto strings = create_strings(num_strings_in_set, length_of_strings);
  if (verbosity_level >= 2) { printf("Done generating strings\n"); }

  bloom_filter<std::string, string_hash> bf(num_bits_in_filter, num_hash_functions);
  for (auto& s : strings) {
    bf.set(s);
  }
  const double density =
    static_cast<double>(bf.count()) / static_cast<double>(num_bits_in_filter);
  printf("%f %zu\n", density, bf.size());
}
@
Each line of code maps to a fairly straightforward idea for our experiment:
We create a [[num_strings_in_set]]-sized collection of distinct strings, each of length [[length_of_strings]].
Then with the [[for]] loop we build our Bloom filter, having initialized it with [[num_bits_in_filter]]-many-bits (all starting at 0 of course), and [[num_hash_functions]] hash-functions.
Having built our Bloom filter, we need ``only'' compare the bits that are set to 1 (via [[count]]) to the total number of bits.
This would complete our experiment run!

\subsection{Bloom Filter False Positivity}
This is one experiment!
<<Bloom Filter False Positive Rate>>=
void false_positive_rate() {
  auto total_strings = num_strings_in_set + num_strings_not_in_set;
  if (verbosity_level >= 1) {
    printf("total_strings: %d\n", total_strings);
  }
  const auto strings = create_strings(total_strings, length_of_strings);
  if (verbosity_level >= 2) {
    printf("Done generating strings\n");
  }
  auto strings_in_set_end = std::begin(strings) + num_strings_in_set;

  bloom_filter<std::string, string_hash> bf(num_bits_in_filter, num_hash_functions);
  std::for_each(std::begin(strings), strings_in_set_end,
                [&bf](auto &s) { bf.set(s); });

  auto false_positives = std::count_if(strings_in_set_end, std::end(strings),
                                       [&bf](auto &v) { return bf.test(v); });
  double fp_rate = double(false_positives) / double(num_strings_not_in_set);
  printf("%f\n", fp_rate);
}
@

\subsection{Accelerating A Set}
We want to time our function.
We can be a little funny and use C++'s RAII thing.
<<Timer Object>>=
class timer_object {
  std::chrono::time_point<std::chrono::steady_clock> start;
  double* elapsed_seconds;
public:
  timer_object(double* elapsed_seconds) {
    this->elapsed_seconds = elapsed_seconds;
    start = std::chrono::steady_clock::now();
  }
  ~timer_object() {
    auto end = std::chrono::steady_clock::now();
    std::chrono::duration<double> elapsed = end - start;
    *elapsed_seconds = elapsed.count();
  }
};
@

We need to set up our two sets to compare:
<<Set Up Application Comparison>>=
auto total_strings = num_strings_in_set + num_strings_not_in_set;
if (verbosity_level >= 1) {
  printf("total_strings: %d\n", total_strings);
}
const auto strings = create_strings(total_strings, length_of_strings);
if (verbosity_level >= 2) { printf("Done generating strings\n"); }
auto strings_in_set_end = std::begin(strings) + num_strings_in_set;

std::set<std::string> base_set;
bloom_filter<std::string, string_hash> bf(num_bits_in_filter, num_hash_functions);
for (auto it = std::begin(strings); it != strings_in_set_end; it++) {
  base_set.insert(*it);
  bf.set(*it);
}

auto to_test_begin = strings_in_set_end;
auto to_test_end = std::end(strings);
@

<<Bloom Filter Application>>=
#include <thread>
void bloom_filter_application() {
  <<Set Up Application Comparison>>
  double seconds = 0;
  size_t count = 0;
  {
    timer_object t(&seconds);
    for (auto it = to_test_begin; it != to_test_end; it++) {
      if (base_set.find(*it) != base_set.end()) {
        count++;
      }
    }
  }
  printf("%f ", seconds);
  assert(!count);

  seconds = 0;
  count = 0;
  {
    timer_object t(&seconds);
    for (auto it = to_test_begin; it != to_test_end; it++) {
      if (bf.test(*it)) {
        if (base_set.find(*it) != base_set.end()) {
          count++;
        }
      }
    }
  }
  printf("%f\n", seconds);
  assert(!count);
}
@

The goal of this experiment is to build some confidence that a Bloom filter provides
a benefit.


\subsection{Driver Plumbing}
Let's begin to dive into those chunks more explicitly.
The various settings we need for our experiments are listed out below.
<<Bloom Filter Settings>>=
double desired_epsilon = 0.0;
int num_strings_in_set = 0;
int num_strings_not_in_set = 0;
int length_of_strings = 0;
int rounding_constant = 0;
  
bool print_help = false;
int verbosity_level = 0;

size_t num_hash_functions = 0;
double num_bits_per_value = 0.0;
size_t num_bits_in_filter = 0;

bool run_density_experiment = false;
bool run_false_positive_experiment = false;
bool run_application_experiment = false;
@
These parameter

We set those settings through command-line parsing:
<<Read Command-Line Parameters>>=
int c;
while ((c = getopt(argc, argv, "dfae:n:x:l:k:b:m:rhv:")) != -1) {
  switch (c) {
    // Which experiments to run:
    case 'd': run_density_experiment = true; break;
    case 'f': run_false_positive_experiment = true; break;
    case 'a': run_application_experiment = true; break;
    
    // Parameters to the experiment
    case 'e': desired_epsilon = std::strtod(optarg, nullptr); break;
    case 'n': num_strings_in_set = std::strtol(optarg, nullptr, 0); break;
    case 'x': num_strings_not_in_set = std::strtol(optarg, nullptr, 0); break;
    case 'l': length_of_strings = std::strtol(optarg, nullptr, 0); break;
    case 'k': num_hash_functions = std::strtol(optarg, nullptr, 0); break; 
    case 'b': num_bits_per_value = std::strtod(optarg, nullptr); break; 
    case 'm': num_bits_in_filter = std::strtol(optarg, nullptr, 0); break; 
    case 'r': rounding_constant = 1; break;

    // Meta-parameters
    case 'h': print_help = true; break;
    case 'v': verbosity_level = std::strtol(optarg, nullptr, 0); break;
  }
}
@
We want to make sure that we aren't asking for anything silly.
There are probably more problems we can find, but this helps address simple typos
<<Error-Check Parameters>>=
if (!run_density_experiment && !run_false_positive_experiment && !run_application_experiment) {
  printf("Error: no requested experiment!\n");
  return 1;
}
if (desired_epsilon < 0 || desired_epsilon >= 1) {
  printf("Error: desired_epsilon must be between 0 and 1, got %f\n", desired_epsilon);
  return 1;
}
if (num_strings_in_set < 0) {
  printf("Error: num_strings_in_set must be non-negative, got %d\n", num_strings_in_set);
  return 1;
}
if (length_of_strings <= 0) {
  printf("Error: length_of_strings must be positive, got %d\n", length_of_strings);
  return 1;
}
if (num_bits_per_value < 0) {
  printf("Error: num_bits_per_value must be positive, got %f\n", num_bits_per_value);
  return 1;
}
@
And then we have optional parameters
We prepare experiments by calculating m and k if they're not specified
<<Prepare Experiments>>=
if (verbosity_level != 0) {
  print_driver_state();
}
if (num_bits_per_value == 0.0) {
  // Specify default to be "optimal".
  num_bits_per_value = -1.44 * std::log2(desired_epsilon);
}
if (num_bits_in_filter == 0) {
  num_bits_in_filter = num_strings_in_set * num_bits_per_value;
}
if (num_hash_functions == 0) {
  if (verbosity_level >= 1) {
    printf("Ideal number of hash-functions pre-rounding: %f\n", -std::log2(desired_epsilon));
  }
  num_hash_functions = size_t(-std::log2(desired_epsilon)) + rounding_constant;
}
if (verbosity_level >= 1) {
  printf("Calculated best bits per value: %f, total number of bits: %ld, number of hash functions: %ld\n",
           num_bits_per_value, num_bits_in_filter, num_hash_functions);
}
@
And then finally we run the experiments we choose:
<<Run Selected Experiments>>=
if (run_density_experiment) {
  optimal_density();
}
if (run_false_positive_experiment) {
  if (num_strings_not_in_set <= 0) {
    printf("Error, for false positive experiment num_strings_not_in_set must be positive, got %d\n",
           num_strings_not_in_set);
    return 1;
  }
  false_positive_rate();
}
if (run_application_experiment) {
  if (num_strings_not_in_set <= 0) {
    printf("Error, for timing experiment num_strings_not_in_set must be positive, got %d\n",
           num_strings_not_in_set);
    return 1;
  }
  bloom_filter_application();
}
@

<<Possible Bloom Filter Experiments>>=
<<Bloom Filter False Positive Rate>>
<<Bloom Filter Optimal Density>>
<<Bloom Filter Application>>
@

<<Driver Utility Functions>>=
<<Hashing Function>>
<<Print Driver Parameters>>
<<Timer Object>>
@

<<Driver Headers>>=
#include <algorithm>
#include <cassert>
#include <chrono>
#include <cmath>
#include <cstdio>
#include <cstdlib>
#include <set>
#include <string>
#include <unistd.h>

#include <iostream>

#include "bloomfilter.h"
#include "datagen.h"
#include "murmur/MurmurHash3.h"
@

<<Print Driver Parameters>>=
void print_driver_state() {
  printf("parameters:\n"
    "\tprint_help=%s\n"
    "\tverbosity_level=%d\n"
    "\trun_density_experiment=%s\n"
    "\trun_false_positive_experiment=%s\n"
    "\trun_application_experiment=%s\n"
    "\tdesired_epsilon=%f\n"
    "\tnum_strings_in_set=%d\n"
    "\tnum_strings_not_in_set=%d\n"
    "\tlength_of_strings=%d\n",
    print_help ? "true" : "false", verbosity_level,
    run_density_experiment ? "true" : "false",
    run_false_positive_experiment ? "true" : "false",
    run_application_experiment ? "true" : "false",
    desired_epsilon, num_strings_in_set, num_strings_not_in_set,
    length_of_strings);
}
@

At this point, we have our analytical results, our Bloom filter implementation,
and a means to exercise that Bloom filter.
Let's get some empirical results!

\section{Experiments}\label{sec:experiments}
We will explore whether our driver successfully confirms theoretical results.

\subsection{Optimal Bit-Density}
One initial theoretical result \cite{TOCITE} points out that any optimal Bloom filter should have a bit-density
(that is, the proportion of bits set to 1) very close to $0.5$.

Can we validate this manifesting in our code, empirically? Yes.
We might want a chart to show that, no matter the specified $\epsilon$, number of input strings, or length of strings,
our default hash-function-count and Bloom-filter-size will yield the desired $\epsilon$ and have density very close.

We're not going to be exhaustive here: this document is to \emph{demonstrate} that theoretical properties can be
empirically observed, not a full validation of our Bloom filter implementation.

So let's make an interesting graph: let's vary $\epsilon$, and chart how density \emph{and} filter-size varies.

<<plot-density.sh>>=
#!/bin/bash
e=0.8
output="$(basename "$0" .sh).fig.tex"
for i in {1..10}; do
  measured=$(./driver -d -e "$e" -n 100000 -l 24 -r)
  echo "$e $measured"
  e=$(echo "$e / 2" | bc -l)
done | gnuplot -e "
  set terminal tikz size 3.5in,2.4in;
  set xlabel 'Specified $\epsilon$';
  set xrange [*:1];
  set logscale x;
  set ylabel 'Bit Density';
  set output '$output';
  plot '-' using 1:2 with linespoints notitle"
@

And this generates the interesting plot we have here:
\begin{figure}[ht]
    \centering
    \input{plot-density.fig.tex}
    \caption{Density by Hash Function Count}
    \label{fig:plot_density}
\end{figure}

<<plot-size.sh>>=
#!/bin/bash
e=0.8
output="$(basename "$0" .sh).fig.tex"
for i in {1..10}; do
  measured=$(./driver -d -e "$e" -n 100000 -l 24 -r)
  echo "$e $measured"
  e=$(echo "$e / 2" | bc -l)
done | gnuplot -e "
  set terminal tikz size 3.5in,2.4in;
  set xlabel 'Specified $\epsilon$';
  set xrange [*:1];
  set logscale x;
  set ylabel 'Filer Size';
  set output '$output';
  plot '-' using 1:3 with linespoints notitle"
@

And this generates the interesting plot we have here:
\begin{figure}[ht]
    \centering
    \input{plot-size.fig.tex}
    \caption{Density by Hash Function Count}
    \label{fig:plot_size}
\end{figure}

This invites us to consider how much it matters to have the ``wrong'' number of hash functions.
<<plot-density-per-hash.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
for hash_count in {1..40}; do
  result=$(./driver -d -e 0.0009 -n 100000 -l 24 -k "$hash_count")
  echo "$hash_count $result"
done | gnuplot -e "
  set terminal tikz size 3.5in,2.4in;
  set yrange [0:1];
  set xlabel 'Number of Hash Functions';
  set ylabel 'Bit Density';
  set output '$output';
  plot '-' using 1:2 with linespoints notitle"
@

And this generates the interesting plot we have here:
\begin{figure}[ht]
    \centering
    \input{plot-density-per-hash.fig.tex}
    \caption{Density by Hash Function Count}
    \label{fig:density_by_hash_function_count}
\end{figure}


\subsection{Specifying Epsilon}
Explore how number-of-bits needed for set-size relates to hash function


\subsection{Measuring Set Acceleration}
Does the total resident memory matter?
<<acceleration-by-count.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
tmpfile=$(mktemp)
for hash_count in {1..10}; do
  count="$hash_count"00000 
  result=$(./driver -a -e 0.01 -n 1000000 -x $count -l 24)
  echo "$count $result"
done > $tmpfile

gnuplot -e "
  set terminal tikz size 5in,2.4in;
  set xlabel 'Number of Excluded Values';
  set format x '$%.0t \cdot 10^{%T}$';
  set ylabel 'Time (Seconds)';
  set output '$output';
  plot '$tmpfile' using 1:2 with linespoints title 'Without BF',
       '$tmpfile' using 1:3 with linespoints title 'With BF'"

rm $tmpfile
@

\begin{figure}[ht]
    \centering
    \input{acceleration-by-count.fig.tex}
    \caption{Comparing Empirical to Theoretical $\epsilon$}
    \label{fig:acceleration_by_count}
\end{figure}

<<acceleration-by-count-big.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
tmpfile=$(mktemp)
for hash_count in {1..10}; do
  count="$hash_count"00000 
  result=$(./driver -a -e 0.01 -n 1000000 -x $count -l 240)
  echo "$count $result"
done > $tmpfile

gnuplot -e "
  set terminal tikz size 5in,2.4in;
  set xlabel 'Number of Excluded Values';
  set format x '$%.0t \cdot 10^{%T}$';
  set ylabel 'Time (Seconds)';
  set output '$output';
  plot '$tmpfile' using 1:2 with linespoints title 'Without BF',
       '$tmpfile' using 1:3 with linespoints title 'With BF'"

rm $tmpfile
@

\begin{figure}[ht]
    \centering
    \input{acceleration-by-count-big.fig.tex}
    \caption{Comparing Empirical to Theoretical $\epsilon$}
    \label{fig:acceleration_by_count_big}
\end{figure}

<<acceleration-by-precision.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
tmpfile=$(mktemp)
e=0.1
for i in {1..10}; do
  result=$(./driver -a -e $e -n 1000000 -x 500000 -l 240)
  e=$(echo "$e / 2" | bc -l)
  echo "$e $result"
done > $tmpfile

gnuplot -e "
  set terminal tikz size 5in,2.4in;
  set xlabel 'Bloom Filter Precison';
  set format x '$%.0t \cdot 10^{%T}$';
  set ylabel 'Time (Seconds)';
  set output '$output';
  plot '$tmpfile' using 1:2 with linespoints title 'Without BF',
       '$tmpfile' using 1:3 with linespoints title 'With BF'"

rm $tmpfile
@

\begin{figure}[ht]
    \centering
    \input{acceleration-by-precision.fig.tex}
    \caption{Comparing Empirical to Theoretical $\epsilon$}
    \label{fig:acceleration_by_precisin}
\end{figure}



\subsection{False Positive Rate}
To what degree does our code empirically match the expected false-positive rate?
Basically, when we specify an epsilon, do we get that epsilon?

This prompts another interesting question: how do we measure its variance?
I don't rightly know.

<<validate-epsilon-empirically.sh>>=
#!/bin/bash
output="$(basename "$0" .sh).fig.tex"
tmpfile=$(mktemp)
e=0.8
for i in {1..10}; do
  measured=$(./driver -f -e "$e" -n 100000 -x 100000 -l 24 -r)
  echo "$e $e $measured"
  e=$(echo "$e / 2" | bc -l)
done > $tmpfile

gnuplot -e "
  set terminal tikz size 3.5in,2.4in;
  set xlabel 'Specified Epsilon';
  set ylabel 'Actual Epsilon';
  set xrange [*:1];
  set logscale x;
  set yrange [*:1];
  set logscale y;
  set key at graph 0.60,0.95;
  set output '$output';
  plot '$tmpfile' using 1:2 with linespoints title 'Expected eps',
       '$tmpfile' using 1:3 with linespoints title 'Measured eps'
  "

rm $tmpfile
@

\begin{figure}[ht]
    \centering
    \input{validate-epsilon-empirically.fig.tex}
    \caption{Comparing Empirical to Theoretical $\epsilon$}
    \label{fig:measure_epsilon}
\end{figure}

Interlude: one thing I realized is that, curiously, with $\epsilon > 0.5$,
the false positive rate was always (at first) exactly $1.0$.
Even for $\epsilon=0.51$ or similar.
In debugging I found that this is because we'd always round down.


\appendix
\section{Library Code Listings}
\subsection{Bloom Filter (bloomfilter.h)}\label{bloomfilterlisting}
\inputminted{c++}{bloomfilter.h}
\subsection{Simple Bit Set (simplebitset.h)}\label{simplebitsetlisting}
\inputminted{c++}{simplebitset.h}
\section{Driver Code Listings}
\subsection{Main Driver}
\inputminted{c++}{driver.cpp}
\subsection{Generating Data}
\inputminted{c++}{datagen.h}
\inputminted{c++}{datagen.cpp}
\end{document}

% Original paper:
% https://dl.acm.org/doi/10.1145/362686.362692
% https://dl.acm.org/doi/pdf/10.1145/362686.362692